{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide covers training, evaluation, and prediction (inference) models in TensorFlow 2.0 in two broad situations:\n",
    "\n",
    "- When using built-in APIs for training & validation (such as `model.fit()`, `model.evaluate()`, `model.predict()`). This is covered in the section **\"Using build-in training & evaluation loops\"**.\n",
    "- When writing custom loops from scratch using eager execution and the `GradientTape` object. This is covered in the section **\"Writing your own training & evaluation loops from scratch\"**.\n",
    "\n",
    "In general, whether you are using built-in loops or writing your own, model training & evaluation works strictly in the same way across every kind of Keras model -- Sequential models, models built with the Functional API, and models written from scratch via model subclassing.\n",
    "\n",
    "This guide doesn't cover distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Using build-in training & evaluation loops\n",
    "\n",
    "When passing data to the built-in training loops of a model, you should either use **Numpy arrays** (if your data is small and fits in memory) or **tf.data Dataset** objects. In the ndex few paragraphs, we'll use the MNIST dataset as Numpy arrays, in order to demonstrate how to use optimizers, losses, and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API overview: a first end-to-end example\n",
    "\n",
    "Let's consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the typical ent-to-end workflow looks like, consisting of training, validation on a holdout set generated from the original training data, and finally evaluation on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 4s 82us/sample - loss: 0.3388 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.1722 - val_sparse_categorical_accuracy: 0.9509\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.1566 - sparse_categorical_accuracy: 0.9530 - val_loss: 0.1427 - val_sparse_categorical_accuracy: 0.9567\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 3s 58us/sample - loss: 0.1143 - sparse_categorical_accuracy: 0.9646 - val_loss: 0.1115 - val_sparse_categorical_accuracy: 0.9670\n",
      "\n",
      "history dict: {'loss': [0.3387888353804638, 0.15656910487708053, 0.11433323673102783], 'sparse_categorical_accuracy': [0.9033, 0.95304, 0.96462], 'val_loss': [0.17223006331379814, 0.1427076998319761, 0.11151026047457017], 'val_sparse_categorical_accuracy': [0.9509, 0.9567, 0.967]}\n",
      "\n",
      "# Evaluate on test data\n",
      "10000/10000 [==============================] - 0s 18us/sample - loss: 0.1072 - sparse_categorical_accuracy: 0.9687\n",
      "test loss, test acc: [0.10719062897669666, 0.9687]\n",
      "\n",
      "# Gerate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load a toy dataset for the sake of this example\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Reverse 10,00 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Specify the training configuration (optimizer, loss, metrics)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              # List of metrics to monitor\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Train the model by slicing the data into \"batches\"\n",
    "# of size \"batch_size\", and repeatedly iterating over\n",
    "# the entire dataset for a given number of \"epochs\"\n",
    "print('# Fit model on training data')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=3,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "print('\\nhistory dict:', history.history)\n",
    "\n",
    "# Evaluation the model on the test data using 'evaluate'\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using 'predict'\n",
    "print('\\n# Gerate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
