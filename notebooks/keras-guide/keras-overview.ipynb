{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tf.keras\n",
    "`tf.keras` is TensorFlow's implementation of the `Keras API specification`. This is a high-level API to build and train models that includes first-class support for TensorFlow-specific functionality, such as `eager execution`, `tf.data` pipelines, and `Estimators`. `tf.keras` makes TensorFlow easier to use without sacrificing flexibility and performance.\n",
    "\n",
    "To get started, import `tf.keras` as part of your TensorFlow program setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras` can run any Keras-compatible code, but keep in mind:\n",
    "\n",
    "The `tf.keras` version in the latest TensorFlow release might not be the same as the latest keras version from PyPI. Check `tf.keras.version`.\n",
    "When `saving a model's weights`, `tf.keras` defaults to the `checkpoint format`. Pass `save_format='h5'` to use HDF5 (or pass a filename that ends in .h5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.4-tf', '2.1.0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple model\n",
    "#### Sequential model\n",
    "In Keras, you assemble *lyaers* to build *models*. A model is (usually) a graph of lyaers. The most common type of model is a stack of layers: `tf.keras.Sequential` model.\n",
    "\n",
    "To build a simple, fully-connected network (i.e. multi-layer perceptron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Add another:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the layers\n",
    "There are many `tf.keras.layers` available. Most of them share some common constructor arguments:\n",
    "\n",
    "- `activation`: Set the activation function for the layer. This parameter is specified by the name of a built-in function or as a callable object. By default, no activation is applied.\n",
    "- `kernel_initializer` and `bias_initializer`: The initialization schemes that create the layer's weights (kernel and bias). This parameter is a name or a callable object. This defaults to the `\"Glorot uniform\"` initializer.\n",
    "- `kernel_regularizer` and `bias_regularizer`: The regularization schemes that apply the layer's weights (kernel and bias), such as L1 or L2 regularization. By default, no regularization is applied.\n",
    "\n",
    "The following instantiates tf.keras.layers.Dense layers using constructor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f8dd7539f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sigmoid layer:\n",
    "layers.Dense(64, activation='sigmoid')\n",
    "# Or:\n",
    "layers.Dense(64, activation=tf.keras.activations.sigmoid)\n",
    "\n",
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
    "\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with a kernel initialized to a random orthogonal matrix:\n",
    "layers.Dense(64, kernel_initializer='orthogonal')\n",
    "\n",
    "# A linear layer with a bias vector initialized to 2.0s:\n",
    "layers.Dense(64, bias_initializer=tf.keras.initializers.Constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate\n",
    "#### Set up training\n",
    "After the model is constructed, configure its learning process by calling the `compile` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    layers.Dense(63, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.Model.compile` takes three important arguments:\n",
    "\n",
    "- `optimizer`: This object specifies the training procedure. Pass it optimizer instances from the `tf.keras.optimizers` module, such as `tf.keras.optimizers.Adam` or `tf.keras.optimizers.SGD`. If you just want to use the default parameters, you can also specify optimizers via strings, such as `'adam'` or `'sgd'`.\n",
    "- `loss`: The function to minimize during optimization. Common choices include mean square error (`mse`), `categorical_crossentropy`, and `binary_crossentropy`. Loss functions are specified by name or by passing a callable object from the `tf.keras.losses` module.\n",
    "- `metrics`: Used to monitor training. These are string names or callables from the `tf.keras.metrics` module.\n",
    "Additionally, to make sure the model trains and evaluates eagerly, you can make sure to pass `run_eagerly=True` as a parameter to compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BinaryCrossentropy',\n",
       " 'CategoricalCrossentropy',\n",
       " 'CategoricalHinge',\n",
       " 'CosineSimilarity',\n",
       " 'Hinge',\n",
       " 'Huber',\n",
       " 'KLD',\n",
       " 'KLDivergence',\n",
       " 'LogCosh',\n",
       " 'Loss',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " 'MeanAbsoluteError',\n",
       " 'MeanAbsolutePercentageError',\n",
       " 'MeanSquaredError',\n",
       " 'MeanSquaredLogarithmicError',\n",
       " 'Poisson',\n",
       " 'Reduction',\n",
       " 'SparseCategoricalCrossentropy',\n",
       " 'SquaredHinge',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_crossentropy',\n",
       " 'categorical_hinge',\n",
       " 'cosine_similarity',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'logcosh',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'serialize',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'squared_hinge']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows a few examples of configuring a model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a model for mean-squared error regression.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Configure a model for categorical classification.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train from NumPy data\n",
    "For small datasets, use in-memory `NumPy` arrays to train and evaluate a model. The model is \"fit\" to the training data using `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 212us/sample - loss: 39.2018 - categorical_accuracy: 0.0980\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 47us/sample - loss: 296.0919 - categorical_accuracy: 0.0960\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 1095.4612 - categorical_accuracy: 0.1110\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 2757.6446 - categorical_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 6179.7515 - categorical_accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 75us/sample - loss: 12462.6728 - categorical_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 17707.5759 - categorical_accuracy: 0.0840\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 26576.2881 - categorical_accuracy: 0.1040\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 32538.0186 - categorical_accuracy: 0.0980\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 37404.6972 - categorical_accuracy: 0.1140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d705e4d68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.Model.fit` takes three important arguments:\n",
    "\n",
    "- `epochs`: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n",
    "- `batch_size`: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n",
    "- `validation_data`: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.\n",
    "\n",
    "Here's an example using `validation_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 51166.4235 - categorical_accuracy: 0.1030 - val_loss: 32734.4218 - val_categorical_accuracy: 0.1100\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 40367.2308 - categorical_accuracy: 0.0980 - val_loss: 39302.3569 - val_categorical_accuracy: 0.1200\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 55623.4503 - categorical_accuracy: 0.1220 - val_loss: 47860.7773 - val_categorical_accuracy: 0.1100\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 60513.1189 - categorical_accuracy: 0.0930 - val_loss: 79568.2546 - val_categorical_accuracy: 0.1200\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 124us/sample - loss: 88207.1187 - categorical_accuracy: 0.1070 - val_loss: 84485.8104 - val_categorical_accuracy: 0.1200\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 95927.7516 - categorical_accuracy: 0.0850 - val_loss: 109583.5739 - val_categorical_accuracy: 0.1100\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 139us/sample - loss: 133155.9035 - categorical_accuracy: 0.1030 - val_loss: 120778.6898 - val_categorical_accuracy: 0.1100\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 131us/sample - loss: 111193.2738 - categorical_accuracy: 0.1040 - val_loss: 175519.7244 - val_categorical_accuracy: 0.1100\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 111855.5893 - categorical_accuracy: 0.0960 - val_loss: 201983.2492 - val_categorical_accuracy: 0.0800\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 132us/sample - loss: 178886.6340 - categorical_accuracy: 0.1020 - val_loss: 182162.8891 - val_categorical_accuracy: 0.0700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d70698ba8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train from tf.data datasets\n",
    "Use the `Dataset API` to scale to large datasets or multi-device training. Pass a `tf.data.Dataset` instance to the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 32 steps\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 199476.3549 - categorical_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 170337.5194 - categorical_accuracy: 0.0960\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 173876.1374 - categorical_accuracy: 0.1100\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 220853.5948 - categorical_accuracy: 0.1130\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 244880.4685 - categorical_accuracy: 0.1060\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 255554.5268 - categorical_accuracy: 0.0800\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 289361.2976 - categorical_accuracy: 0.0920\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 297669.1155 - categorical_accuracy: 0.1010\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 350329.4334 - categorical_accuracy: 0.1040\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 317199.5047 - categorical_accuracy: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d683d46d8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates a toy dataset instance:\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `Dataset` yields batches of data, this snippet does not require a `batch_size`.\n",
    "\n",
    "Datasets can also be used for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 32 steps, validate for 4 steps\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 371637.1958 - categorical_accuracy: 0.0980 - val_loss: 417975.1429 - val_categorical_accuracy: 0.1200\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 352619.4128 - categorical_accuracy: 0.1050 - val_loss: 276694.2941 - val_categorical_accuracy: 0.1100\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 376465.8815 - categorical_accuracy: 0.0980 - val_loss: 326388.9991 - val_categorical_accuracy: 0.0500\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 361749.9208 - categorical_accuracy: 0.1070 - val_loss: 323306.5435 - val_categorical_accuracy: 0.1100\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 414727.1478 - categorical_accuracy: 0.0980 - val_loss: 492189.3892 - val_categorical_accuracy: 0.1100\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 558222.2509 - categorical_accuracy: 0.0950 - val_loss: 447870.7025 - val_categorical_accuracy: 0.1500\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 554148.5678 - categorical_accuracy: 0.0910 - val_loss: 427824.2552 - val_categorical_accuracy: 0.1200\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 518255.8315 - categorical_accuracy: 0.1100 - val_loss: 622815.8386 - val_categorical_accuracy: 0.0800\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 580840.0688 - categorical_accuracy: 0.0960 - val_loss: 467847.1154 - val_categorical_accuracy: 0.1100\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 608752.0387 - categorical_accuracy: 0.0940 - val_loss: 456828.9661 - val_categorical_accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d78026320>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "\n",
    "model.fit(dataset, epochs=10,\n",
    "          validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and predict\n",
    "The `tf.keras.Model.evaluate` and `tf.keras.Model.predict` methods can use NumPy data and a `tf.data.Dataset`.\n",
    "\n",
    "Here's how to *evaluate* the inference-mode loss and metrics for the data provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 35us/sample - loss: 477805.7111 - categorical_accuracy: 0.1090\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 477809.5419 - categorical_accuracy: 0.1090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[477809.54194641113, 0.109]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Numpy arrays\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "# With a Dataset\n",
    "model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's how to *predict* the output of the last layer in inference for the data provided, as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(data, batch_size=32)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build complex models\n",
    "#### The Functional API\n",
    "The `tf.keras.Sequential` model is a simple stack of layers that cannot represent arbitrary models. Use the `Keras functional API` to build complex model topologies such as:\n",
    "\n",
    "- Multi-input models,\n",
    "- Multi-output models,\n",
    "- Models with shared layers (the same layer called several times),\n",
    "- Models with non-sequential data flows (e.g. residual connections).\n",
    "\n",
    "Building a model with the functional API works like this:\n",
    "\n",
    "1. A layer instance is callable and returns a tensor.\n",
    "2. Input tensors and output tensors are used to define a `tf.keras.Model` instance.\n",
    "3. This model is trained just like the `Sequential` model.\n",
    "\n",
    "The following example uses the functional API to build a simple, fully-connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))  # Returns an input placeholder\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model given inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 260us/sample - loss: 13.6606 - accuracy: 0.1200\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 22.6619 - accuracy: 0.1160\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 107us/sample - loss: 36.7885 - accuracy: 0.0990\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 129us/sample - loss: 54.1488 - accuracy: 0.1220\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 125us/sample - loss: 72.7108 - accuracy: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d7023f1d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model subclassing\n",
    "Build a fully-customizable model by subclassing `tf.keras.Model` and defining your own forward pass. Create layers in the `__init__` method and set them as attributes of the class instance. Define the forward pass in the `call` method.\n",
    "\n",
    "Model subclassing is particularly useful when `eager excution` is enabled, because it allows the forward pass to be written imperatively.\n",
    "\n",
    "The following example shows a subclassed `tf.keras.Model` using a custom forward pass that does not have to be run imperatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # Define your layers here.\n",
    "        self.dense_1 = layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = layers.Dense(10, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Define your forward pass here.\n",
    "        # using layers you previously defined (in '__init__').\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the new model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 242us/sample - loss: 11.5569 - accuracy: 0.0990\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 11.5394 - accuracy: 0.1020\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 11.5337 - accuracy: 0.1130\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 11.5304 - accuracy: 0.1060\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 11.5277 - accuracy: 0.1030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d68123710>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom layers\n",
    "Create a custom layer by subclassing `tf.keras.layers.Layer` and implementing the following methods:\n",
    "\n",
    "- `__init__`: Optionally define sublayers to be used by this layer.\n",
    "- `build`: Create the weights of the layer. Add weights with the `add_weight` method.\n",
    "- `call`: Define the forward pass.\n",
    "- Optionally, a layer can be serialized by implementing the `get_config` method and the `from_config` class method.\n",
    "\n",
    "Here's an example of a custom layer that implements a `matmul` of an input with a kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return csl(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model using your custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 11.5598 - accuracy: 0.0950\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 11.5598 - accuracy: 0.0940\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 11.5597 - accuracy: 0.0930\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 43us/sample - loss: 11.5592 - accuracy: 0.0980\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 53us/sample - loss: 11.5593 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d68053438>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    MyLayer(10),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "# The compile step sepcifies the training configuration\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_2 (MyLayer)         multiple                  320       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "A callback is an object passed to a model to customize and extend its behavior during training. You can write your own custom callback, or use the built-in `tf.keras.callbacks` that include:\n",
    "\n",
    "- `tf.keras.callbacks.ModelCheckpoint`: Save checkpoints of your model at regular intervals.\n",
    "- `tf.keras.callbacks.LearningRateScheduler`: Dynamically change the learning rate.\n",
    "- `tf.keras.callbacks.EarlyStopping`: Interrupt training when validation performance has stopped improving.\n",
    "- `tf.keras.callbacks.TensorBoard`: Monitor the model's behavior using TensorBoard.\n",
    "\n",
    "To use a `tf.keras.callbacks.Callback`, pass in to the model's `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 116us/sample - loss: 11.5593 - accuracy: 0.0950 - val_loss: 11.3389 - val_accuracy: 0.0600\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 11.5588 - accuracy: 0.1000 - val_loss: 11.3398 - val_accuracy: 0.0300\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 11.5585 - accuracy: 0.0960 - val_loss: 11.3383 - val_accuracy: 0.0500\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 11.5578 - accuracy: 0.1010 - val_loss: 11.3396 - val_accuracy: 0.0600\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 11.5586 - accuracy: 0.0970 - val_loss: 11.3385 - val_accuracy: 0.0400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d380dd0b8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    # Interrupt training if 'val_loss' stops improving for over 2 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    # Write TensorBoard logs to './logs' directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and restore\n",
    "#### Save just the weights values\n",
    "Save and load the weights of a model using `tf.keras.Model.save_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8d3821a748>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights to a TensorFlow Checkpoint file\n",
    "model.save_weights('./weights/my_model')\n",
    "\n",
    "# Restore the model's state,\n",
    "# This requires a model with the same architecture.\n",
    "model.load_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this saves the model's weights in the `TensorFlow checkpoint` file format. Weights can also be saved to the Keras HDF5 format (the default for the multi-backend implementation of Keras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t      my_model.data-00001-of-00002\r\n",
      "my_model.data-00000-of-00002  my_model.index\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to a HDF5 file\n",
    "model.save_weights('my_model.h5', save_format='h5')\n",
    "\n",
    "# Restore the model's state\n",
    "model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-overview.ipynb  logs  my_model.h5  weights\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save just the model configuration\n",
    "A model's configuration can be saved—this serializes the model architecture without any weights. A saved configuration can recreate and initialize the same model, even without the code that defined the original model. Keras supports JSON and YAML serialization formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backend': 'tensorflow',\n",
      " 'class_name': 'Sequential',\n",
      " 'config': {'layers': [{'class_name': 'Dense',\n",
      "                        'config': {'activation': 'relu',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'batch_input_shape': [None, 32],\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float64',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_26',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 64,\n",
      "                                   'use_bias': True}},\n",
      "                       {'class_name': 'Dense',\n",
      "                        'config': {'activation': 'softmax',\n",
      "                                   'activity_regularizer': None,\n",
      "                                   'bias_constraint': None,\n",
      "                                   'bias_initializer': {'class_name': 'Zeros',\n",
      "                                                        'config': {}},\n",
      "                                   'bias_regularizer': None,\n",
      "                                   'dtype': 'float64',\n",
      "                                   'kernel_constraint': None,\n",
      "                                   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
      "                                                          'config': {'seed': None}},\n",
      "                                   'kernel_regularizer': None,\n",
      "                                   'name': 'dense_27',\n",
      "                                   'trainable': True,\n",
      "                                   'units': 10,\n",
      "                                   'use_bias': True}}],\n",
      "            'name': 'sequential_6'},\n",
      " 'keras_version': '2.2.4-tf'}\n"
     ]
    }
   ],
   "source": [
    "# Serialize a model to JSON format\n",
    "json_string = model.to_json()\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "pprint.pprint(json.loads(json_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the model (newly initialized) from the JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_model = tf.keras.models.model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing a model to YAML format requires that you install `pyyaml` before you import TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "  layers:\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: relu\n",
      "      activity_regularizer: null\n",
      "      batch_input_shape: !!python/tuple\n",
      "      - null\n",
      "      - 32\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float64\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_26\n",
      "      trainable: true\n",
      "      units: 64\n",
      "      use_bias: true\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: softmax\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      dtype: float64\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: GlorotUniform\n",
      "        config:\n",
      "          seed: null\n",
      "      kernel_regularizer: null\n",
      "      name: dense_27\n",
      "      trainable: true\n",
      "      units: 10\n",
      "      use_bias: true\n",
      "  name: sequential_6\n",
      "keras_version: 2.2.4-tf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yaml_string = model.to_yaml()\n",
    "print(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the model from the YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkpark/.pyenv/versions/3.7.1/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py:76: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "fresh_model = tf.keras.models.model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**: *Subclassed models are not serializable because their architecture is defined by the Python code in the body of the `call` method.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sasve the entire model in one file\n",
    "The entire model can be saved to file that contains the weights values, the model's configuration, and even the optimizer's configuration. This allows you to checkpoint a model and resume training later—from the excat same state—without access to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 224us/sample - loss: 11.5460 - accuracy: 0.0980\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 11.5473 - accuracy: 0.0970\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 11.5723 - accuracy: 0.0940\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 11.6175 - accuracy: 0.0920\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 11.6525 - accuracy: 0.0950\n"
     ]
    }
   ],
   "source": [
    "# Create a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(10, activation='softmax', input_shape=(32,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, batch_size=32, epochs=5)\n",
    "\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager execution\n",
    "`Eager execution` is an impreative programming environment that evaluates operations immediately. This is not required for Keras, but is supported by `tf.keras` and useful for inspecting your program and debugging.\n",
    "\n",
    "All of the `tf.keras` model-building APIs are compatible with eager execution. And while the `Sequential` and functional APIs can be used, eager execution especially benefits *model subclassing* and building *custom layers*—the APIs that require you to write the forward pass as code (instead of the APIs that create model by assembling existing layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution\n",
    "#### Multiple GPUs\n",
    "`tf.keras` models can run on multiple GPUs using `tf.distribute.Strategy`. This API provides distributed training on multiple GPUs with almost no changes to existing code.\n",
    "\n",
    "Currently, `tf.distribute.MirroredStrategy` is the only supported distribution strategy. `MirroredStrategy` does in-graph replication with synchronous training using all-reduce on a single machine. To use `distribute.Strategys` , nest the optimizer instantiation and model construction and compilation in a `Strategy`'s `.scope()`, then train the model.\n",
    "\n",
    "The following example distributes a `tf.keras.Model` across multiple GPUs on a single machine.\n",
    "\n",
    "First, define a model inside the distributed strategy scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(0.2)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on data as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 32 steps\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d782880f0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1024, 10))\n",
    "y = np.random.randint(2, size=(1024, 1))\n",
    "x = tf.cast(x, tf.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "model.fit(dataset, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
